{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Tarea 1: Classificaton Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets fsspec\n",
        "!pip install -U transformers datasets evaluate accelerate --no-cache-dir\n",
        "!pip install -U transformers --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16r1aHeLGSFR",
        "outputId": "948d7e48-2ee5-49c7-e89e-6086b1017515"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUOxOs_-zlVJ",
        "outputId": "41c26fca-e652-4f09-e325-4724658672bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available: False\n",
            "CUDA version: 12.4\n",
            "Number of GPUs available: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "TAPAS models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version. Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n",
            "GroupViT models are not usable since `tensorflow_probability` can't be loaded. It seems you have `tensorflow_probability` installed with the wrong tensorflow version.Please try to reinstall it following the instructions here: https://github.com/tensorflow/probability.\n"
          ]
        }
      ],
      "source": [
        "# Librer铆as\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "import torch\n",
        "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "\n",
        "from time import time\n",
        "from datasets import *\n",
        "from transformers import *\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhTfthoTfFW"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "El split **MNLI** del dataset **GLUE** consiste en un par de oraciones (premisa e hip贸tesis) y una etiqueta indicando la relaci贸n entre ellas:\n",
        "\n",
        "- _Entailment_: La hip贸tesis es una conclusi贸n l贸gica de la premisa.\n",
        "- _Neutral_: La hip贸tesis no puede ser determinada como verdadera o falsa basada en la premisa.\n",
        "- _Contradiction_: La hip贸tesis contradice la premisa.\n",
        "\n",
        "Adem谩s, este split contiene diferentes subconjuntos. Principalmente, usaremos el de _train_ para entrenar y los de _validation_ para evaluar la calidad del modelo. Los de _test_ los omitiremos para este trabajo.\n",
        "- _Train_: Dataset que usaremos para entrenar el modelo.\n",
        "- _MNLI-matched_ (MNLI-m): Dataset de validaci贸n creado a partir de las mismas categor铆as de los del conjunto de entrenamiento (e.g., noticias, ficci贸n).\n",
        "- _MNLI-mismatched_ (MNLI-mm): Dataset de validaci贸n creado a partir de diferentes categor铆as de los del conjunto de entrenamiento (e.g., discursos pol铆ticos, cartas).\n",
        "\n",
        "Aqu铆 la ficha del dataset para que pod谩is explorarla: https://huggingface.co/datasets/nyu-mll/glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_AY1ATSIrIq",
        "outputId": "b031abbc-66d6-4346-ccba-c464cdb23d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 392702\n",
              "    })\n",
              "    validation_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9815\n",
              "    })\n",
              "    validation_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9832\n",
              "    })\n",
              "    test_matched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9796\n",
              "    })\n",
              "    test_mismatched: Dataset({\n",
              "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "        num_rows: 9847\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPR_1LBRzlVN"
      },
      "source": [
        "Con el 煤nico motivo de no demorar los tiempos de entrenamiento. Filtraremos el dataset y nos quedaremos solo con los registros que tenga longitud del campo _premise_ inferior a 20.\n",
        "\n",
        "El resto de la pr谩ctica se pide trabajarla sobre la variable `ds_tarea`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9f54219c1caa4732924828b945591c37"
          ]
        },
        "id": "X6HrpprwIrIz",
        "outputId": "ce19e508-3856-4069-8d00-c49fb44afd26"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/392702 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f54219c1caa4732924828b945591c37"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def filter_rows(x):\n",
        "    return len(x['premise'])<20\n",
        "ds_tarea = dataset.filter(filter_rows)\n",
        "\n",
        "assert len(ds_tarea['train']) == 13635\n",
        "assert len(ds_tarea['validation_matched']) == 413\n",
        "assert len(ds_tarea['validation_mismatched']) == 296\n",
        "\n",
        "ds_tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPepm7npTfFZ"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "En este apartado es donde tendr茅is que realizar todo el trabajo de la pr谩ctica. El formato, el an谩lisis, el modelo escogido y cualquier proceso intermedio que consider茅is es totalmente libre. Sin embargo, hay algunas pautas que tendr茅is que cumplir:\n",
        "\n",
        "- La variable `model_checkpoint` debe almacenar el nombre del modelo y el tokenizador de  que vais a utilizar.\n",
        "- La variable `model` y la variable `tokenizer` almacenar谩n, respectivamente, el modelo y el tokenizador de  que vais a utilizar.\n",
        "- La variable `trainer` almacenar谩 el _Trainer_ de  que, en la siguiente secci贸n utilizar茅is para entrenar el modelo.\n",
        "- Debe existir una funci贸n llamada `preprocess_function` que realice la tokenizaci贸n y, si lo consider谩is oportuno, transformaciones de las _features_.\n",
        "\n",
        "Nota: En el _tokenizer_, es obligatorio que el argumento `padding` sea distinto de `False` y que su salida sean **tensores** de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86Z-p6BQzlVO"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = None\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "def preprocess_function(x):\n",
        "    return x\n",
        "\n",
        "trainer = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL0JavjMzlVP"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['premise'], examples['hypothesis'],\n",
        "                     truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "encoded_ds = ds_tarea.map(preprocess_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IgT61yRzlVP"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPObTUrRzlVP"
      },
      "outputs": [],
      "source": [
        "# Definimos el trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_ds[\"train\"],\n",
        "    eval_dataset=encoded_ds[\"validation_matched\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=lambda p: {\n",
        "        \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "start = time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end = time()\n",
        "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K81xruTmzlVQ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO91AUnLzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "print(f\"**** EVALUACIN ****\")\n",
        "print(f\"********\\nTokenizer config:\\n{tokenizer}\")\n",
        "print(f\"\\n\\n********\\nModel config:\\n{model.config}\")\n",
        "print(f\"\\n\\n********\\nTrainer arguments:\\n{trainer.args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGn0uMKEzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "sample = ds_tarea['validation_matched'][0]\n",
        "inputs = preprocess_function(sample)\n",
        "for key, value in inputs.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key} es una instancia de torch.Tensor\")\n",
        "    else:\n",
        "        print(f\"{key} no es una instancia de torch.Tensor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ComDZMpPzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def predict(x):\n",
        "    inputs = preprocess_function(x)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        return {'prediction': predictions.item()}\n",
        "\n",
        "ds_predictions = ds_tarea.map(predict)\n",
        "\n",
        "assert len(ds_predictions['train']) == 13635\n",
        "assert len(ds_predictions['validation_matched']) == 413\n",
        "assert len(ds_predictions['validation_mismatched']) == 296\n",
        "\n",
        "ds_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oklnJNG1zlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "    print(f\"*** {subset} ***\")\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hIfI0QArzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "metrics = {}\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    pre = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    rec = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    metrics[subset] = [acc] + pre.tolist() + rec.tolist()\n",
        "    print(f\"Subset: {subset}:\")\n",
        "    print(f\"Accuracy: {acc:.2f} | Precision0: {pre[0]:.2f} | Precision1: {pre[1]:.2f} | Precision2: {pre[2]:.2f} | Recall0: {rec[0]:.2f} | Recall1: {rec[1]:.2f} | Recall2: {rec[2]:.2f}\")\n",
        "    print(\"-----\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNN49U0azlVR"
      },
      "source": [
        "### Criterio de evaluaci贸n\n",
        "\n",
        "La **nota final de la tarea1** estar谩 relacionada con el resultado de las m茅tricas de vuestro modelo en la combinaci贸n de *accuracy*, *precision* y *recall* para cada _split_ de datos.\n",
        "\n",
        "El criterio de evaluaci贸n ser谩 el siguiente:\n",
        "- La tarea1 se aprobar谩 si el notebook se entrega sin fallos y con un modelo entrenado (independientemente de sus m茅tricas).\n",
        "- La tarea1 tiene un 10 si se cumple que las m茅tricas de vuestro modelo entrenado igualan o superan los siguientes umbrales:\n",
        "\n",
        "| Subset               | Accuracy | Precision0 | Precision1 | Precision2 | Recall0 | Recall1 | Recall2 |\n",
        "|----------------------|----------|------------|------------|------------|---------|---------|---------|\n",
        "| validation_matched    | 0.78     | 0.78       | 0.76       | 0.85       | 0.80    | 0.77    | 0.81    |\n",
        "| validation_mismatched | 0.79     | 0.70       | 0.70       | 0.70       | 0.65    | 0.71    | 0.85    |\n",
        "\n",
        "- Por cada valor inferior a dicha m茅trica, la tarea pierde 0.5 puntos (m谩ximo 5.0 puntos de p茅rdida).\n",
        "\n",
        "Nota: La nota que se calcula a continuaci贸n es orientativa y podr铆a verse reducida en funci贸n del c贸digo de la entrega."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1PhwP-lzlVR"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def calculo_nota(metric):\n",
        "\n",
        "    vm_acc = float(metric['validation_matched'][0])\n",
        "    vm_pre0 = float(metric['validation_matched'][1])\n",
        "    vm_pre1 = float(metric['validation_matched'][2])\n",
        "    vm_pre2 = float(metric['validation_matched'][3])\n",
        "    vm_rec0 = float(metric['validation_matched'][4])\n",
        "    vm_rec1 = float(metric['validation_matched'][5])\n",
        "    vm_rec2 = float(metric['validation_matched'][6])\n",
        "    vmm_acc = float(metric['validation_mismatched'][0])\n",
        "    vmm_pre0 = float(metric['validation_mismatched'][1])\n",
        "    vmm_pre1 = float(metric['validation_mismatched'][2])\n",
        "    vmm_pre2 = float(metric['validation_mismatched'][3])\n",
        "    vmm_rec0 = float(metric['validation_mismatched'][4])\n",
        "    vmm_rec1 = float(metric['validation_mismatched'][5])\n",
        "    vmm_rec2 = float(metric['validation_mismatched'][6])\n",
        "\n",
        "    thresholds = {\n",
        "        'vm_acc': 0.78, 'vm_pre0': 0.78, 'vm_pre1': 0.76, 'vm_pre2': 0.85,\n",
        "        'vm_rec0': 0.80, 'vm_rec1': 0.77, 'vm_rec2': 0.81,\n",
        "        'vmm_acc': 0.79, 'vmm_pre0': 0.70, 'vmm_pre1': 0.70, 'vmm_pre2': 0.70,\n",
        "        'vmm_rec0': 0.65, 'vmm_rec1': 0.71, 'vmm_rec2': 0.85,\n",
        "    }\n",
        "    values = {\n",
        "        'vm_acc': vm_acc, 'vm_pre0': vm_pre0, 'vm_pre1': vm_pre1, 'vm_pre2': vm_pre2,\n",
        "        'vm_rec0': vm_rec0, 'vm_rec1': vm_rec1, 'vm_rec2': vm_rec2,\n",
        "        'vmm_acc': vmm_acc, 'vmm_pre0': vmm_pre0, 'vmm_pre1': vmm_pre1, 'vmm_pre2': vmm_pre2,\n",
        "        'vmm_rec0': vmm_rec0, 'vmm_rec1': vmm_rec1, 'vmm_rec2': vmm_rec2,\n",
        "    }\n",
        "\n",
        "    nota = 10\n",
        "    for key in thresholds:\n",
        "        if values[key] < thresholds[key]:\n",
        "            nota -= 0.5\n",
        "    return max(nota, 5.0)\n",
        "\n",
        "print(f\"Tu nota de la tarea1 es: {calculo_nota(metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PdB3k62zlVR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (glue_env)",
      "language": "python",
      "name": "glue_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f54219c1caa4732924828b945591c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19facadc9e59410c8368a0bda0a68cd7",
              "IPY_MODEL_d14119b5113d452fb29ccee43fe11afe",
              "IPY_MODEL_8b8eebaee5c2442baba158d23c875970"
            ],
            "layout": "IPY_MODEL_8e1080b49d31425eb1793847128d8f30"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}