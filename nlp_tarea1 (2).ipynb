{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yulz72/NLP/blob/main/nlp_tarea1%20(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Tarea 1: Classificaton Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets fsspec\n",
        "!pip install --upgrade transformers datasets\n",
        "!pip install -U transformers datasets evaluate accelerate --no-cache-dir\n"
      ],
      "metadata": {
        "id": "4uohSjd0Fp1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUOxOs_-zlVJ"
      },
      "outputs": [],
      "source": [
        "# Librerías\n",
        "\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "import torch\n",
        "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
        "\n",
        "from time import time\n",
        "from datasets import *\n",
        "from transformers import *\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvhTfthoTfFW"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "El split **MNLI** del dataset **GLUE** consiste en un par de oraciones (premisa e hipótesis) y una etiqueta indicando la relación entre ellas:\n",
        "\n",
        "- _Entailment_: La hipótesis es una conclusión lógica de la premisa.\n",
        "- _Neutral_: La hipótesis no puede ser determinada como verdadera o falsa basada en la premisa.\n",
        "- _Contradiction_: La hipótesis contradice la premisa.\n",
        "\n",
        "Además, este split contiene diferentes subconjuntos. Principalmente, usaremos el de _train_ para entrenar y los de _validation_ para evaluar la calidad del modelo. Los de _test_ los omitiremos para este trabajo.\n",
        "- _Train_: Dataset que usaremos para entrenar el modelo.\n",
        "- _MNLI-matched_ (MNLI-m): Dataset de validación creado a partir de las mismas categorías de los del conjunto de entrenamiento (e.g., noticias, ficción).\n",
        "- _MNLI-mismatched_ (MNLI-mm): Dataset de validación creado a partir de diferentes categorías de los del conjunto de entrenamiento (e.g., discursos políticos, cartas).\n",
        "\n",
        "Aquí la ficha del dataset para que podáis explorarla: https://huggingface.co/datasets/nyu-mll/glue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_AY1ATSIrIq"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"mnli\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPR_1LBRzlVN"
      },
      "source": [
        "Con el único motivo de no demorar los tiempos de entrenamiento. Filtraremos el dataset y nos quedaremos solo con los registros que tenga longitud del campo _premise_ inferior a 20.\n",
        "\n",
        "El resto de la práctica se pide trabajarla sobre la variable `ds_tarea`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6HrpprwIrIz"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def filter_rows(x):\n",
        "    return len(x['premise'])<20\n",
        "ds_tarea = dataset.filter(filter_rows)\n",
        "\n",
        "assert len(ds_tarea['train']) == 13635\n",
        "assert len(ds_tarea['validation_matched']) == 413\n",
        "assert len(ds_tarea['validation_mismatched']) == 296\n",
        "\n",
        "ds_tarea"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPepm7npTfFZ"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "En este apartado es donde tendréis que realizar todo el trabajo de la práctica. El formato, el análisis, el modelo escogido y cualquier proceso intermedio que consideréis es totalmente libre. Sin embargo, hay algunas pautas que tendréis que cumplir:\n",
        "\n",
        "- La variable `model_checkpoint` debe almacenar el nombre del modelo y el tokenizador de 🤗 que vais a utilizar.\n",
        "- La variable `model` y la variable `tokenizer` almacenarán, respectivamente, el modelo y el tokenizador de 🤗 que vais a utilizar.\n",
        "- La variable `trainer` almacenará el _Trainer_ de 🤗 que, en la siguiente sección utilizaréis para entrenar el modelo.\n",
        "- Debe existir una función llamada `preprocess_function` que realice la tokenización y, si lo consideráis oportuno, transformaciones de las _features_.\n",
        "\n",
        "Nota: En el _tokenizer_, es obligatorio que el argumento `padding` sea distinto de `False` y que su salida sean **tensores** de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL0JavjMzlVP"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['premise'],\n",
        "        examples['hypothesis'],\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "encoded_ds = ds_tarea.map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IgT61yRzlVP"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "small_train = encoded_ds[\"train\"].select(range(200))\n",
        "small_eval = encoded_ds[\"validation_matched\"].select(range(50))\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train,\n",
        "    eval_dataset=small_eval,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "TuvFwtuY4qQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "start = time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "end = time()\n",
        "print(f\">>>>>>>>>>>>> elapsed time: {(end-start)/60:.0f}m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K81xruTmzlVQ"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO91AUnLzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "print(f\"**** EVALUACIÓN ****\")\n",
        "print(f\"********\\nTokenizer config:\\n{tokenizer}\")\n",
        "print(f\"\\n\\n********\\nModel config:\\n{model.config}\")\n",
        "print(f\"\\n\\n********\\nTrainer arguments:\\n{trainer.args}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGn0uMKEzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "sample = ds_tarea['validation_matched'][0]\n",
        "inputs = preprocess_function(sample)\n",
        "for key, value in inputs.items():\n",
        "    if isinstance(value, torch.Tensor):\n",
        "        print(f\"{key} es una instancia de torch.Tensor\")\n",
        "    else:\n",
        "        print(f\"{key} no es una instancia de torch.Tensor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ComDZMpPzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def predict(x):\n",
        "    inputs = preprocess_function(x)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        return {'prediction': predictions.item()}\n",
        "\n",
        "ds_predictions = ds_tarea.map(predict)\n",
        "\n",
        "assert len(ds_predictions['train']) == 13635\n",
        "assert len(ds_predictions['validation_matched']) == 413\n",
        "assert len(ds_predictions['validation_mismatched']) == 296\n",
        "\n",
        "ds_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oklnJNG1zlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
        "    print(f\"*** {subset} ***\")\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hIfI0QArzlVQ"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "metrics = {}\n",
        "for subset in ['train', 'validation_matched', 'validation_mismatched']:\n",
        "    y_true = ds_predictions[subset]['label']\n",
        "    y_pred = ds_predictions[subset]['prediction']\n",
        "    acc = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    pre = precision_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    rec = recall_score(y_true=y_true, y_pred=y_pred, average=None)\n",
        "    metrics[subset] = [acc] + pre.tolist() + rec.tolist()\n",
        "    print(f\"Subset: {subset}:\")\n",
        "    print(f\"Accuracy: {acc:.2f} | Precision0: {pre[0]:.2f} | Precision1: {pre[1]:.2f} | Precision2: {pre[2]:.2f} | Recall0: {rec[0]:.2f} | Recall1: {rec[1]:.2f} | Recall2: {rec[2]:.2f}\")\n",
        "    print(\"-----\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNN49U0azlVR"
      },
      "source": [
        "### Criterio de evaluación\n",
        "\n",
        "La **nota final de la tarea1** estará relacionada con el resultado de las métricas de vuestro modelo en la combinación de *accuracy*, *precision* y *recall* para cada _split_ de datos.\n",
        "\n",
        "El criterio de evaluación será el siguiente:\n",
        "- La tarea1 se aprobará si el notebook se entrega sin fallos y con un modelo entrenado (independientemente de sus métricas).\n",
        "- La tarea1 tiene un 10 si se cumple que las métricas de vuestro modelo entrenado igualan o superan los siguientes umbrales:\n",
        "\n",
        "| Subset               | Accuracy | Precision0 | Precision1 | Precision2 | Recall0 | Recall1 | Recall2 |\n",
        "|----------------------|----------|------------|------------|------------|---------|---------|---------|\n",
        "| validation_matched    | 0.78     | 0.78       | 0.76       | 0.85       | 0.80    | 0.77    | 0.81    |\n",
        "| validation_mismatched | 0.79     | 0.70       | 0.70       | 0.70       | 0.65    | 0.71    | 0.85    |\n",
        "\n",
        "- Por cada valor inferior a dicha métrica, la tarea pierde 0.5 puntos (máximo 5.0 puntos de pérdida).\n",
        "\n",
        "Nota: La nota que se calcula a continuación es orientativa y podría verse reducida en función del código de la entrega."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1PhwP-lzlVR"
      },
      "outputs": [],
      "source": [
        "# No modificar esta celda\n",
        "# Esta celda, celda tiene que estar ejecutada en la entrega\n",
        "\n",
        "def calculo_nota(metric):\n",
        "\n",
        "    vm_acc = float(metric['validation_matched'][0])\n",
        "    vm_pre0 = float(metric['validation_matched'][1])\n",
        "    vm_pre1 = float(metric['validation_matched'][2])\n",
        "    vm_pre2 = float(metric['validation_matched'][3])\n",
        "    vm_rec0 = float(metric['validation_matched'][4])\n",
        "    vm_rec1 = float(metric['validation_matched'][5])\n",
        "    vm_rec2 = float(metric['validation_matched'][6])\n",
        "    vmm_acc = float(metric['validation_mismatched'][0])\n",
        "    vmm_pre0 = float(metric['validation_mismatched'][1])\n",
        "    vmm_pre1 = float(metric['validation_mismatched'][2])\n",
        "    vmm_pre2 = float(metric['validation_mismatched'][3])\n",
        "    vmm_rec0 = float(metric['validation_mismatched'][4])\n",
        "    vmm_rec1 = float(metric['validation_mismatched'][5])\n",
        "    vmm_rec2 = float(metric['validation_mismatched'][6])\n",
        "\n",
        "    thresholds = {\n",
        "        'vm_acc': 0.78, 'vm_pre0': 0.78, 'vm_pre1': 0.76, 'vm_pre2': 0.85,\n",
        "        'vm_rec0': 0.80, 'vm_rec1': 0.77, 'vm_rec2': 0.81,\n",
        "        'vmm_acc': 0.79, 'vmm_pre0': 0.70, 'vmm_pre1': 0.70, 'vmm_pre2': 0.70,\n",
        "        'vmm_rec0': 0.65, 'vmm_rec1': 0.71, 'vmm_rec2': 0.85,\n",
        "    }\n",
        "    values = {\n",
        "        'vm_acc': vm_acc, 'vm_pre0': vm_pre0, 'vm_pre1': vm_pre1, 'vm_pre2': vm_pre2,\n",
        "        'vm_rec0': vm_rec0, 'vm_rec1': vm_rec1, 'vm_rec2': vm_rec2,\n",
        "        'vmm_acc': vmm_acc, 'vmm_pre0': vmm_pre0, 'vmm_pre1': vmm_pre1, 'vmm_pre2': vmm_pre2,\n",
        "        'vmm_rec0': vmm_rec0, 'vmm_rec1': vmm_rec1, 'vmm_rec2': vmm_rec2,\n",
        "    }\n",
        "\n",
        "    nota = 10\n",
        "    for key in thresholds:\n",
        "        if values[key] < thresholds[key]:\n",
        "            nota -= 0.5\n",
        "    return max(nota, 5.0)\n",
        "\n",
        "print(f\"Tu nota de la tarea1 es: {calculo_nota(metrics)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PdB3k62zlVR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python (glue_env)",
      "language": "python",
      "name": "glue_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}